{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"LSTM for time series classification\n",
    "Made: 30 march 2016\n",
    "\n",
    "This model takes in time series and class labels.\n",
    "The LSTM models the time series. A fully-connected layer\n",
    "generates an output to be classified with Softmax\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    print(\"Input Shape of data\", data.shape)\n",
    "    formatted_data = np.reshape(data,[data.shape[0]/30,30,data.shape[1]])\n",
    "    print(\"Output Shape of data\", formatted_data.shape)\n",
    "    return formatted_data\n",
    "    \n",
    "def sample_batch(X_train,y_train,batch_size,num_steps):\n",
    "    \"\"\" Function to sample a batch for training\"\"\"\n",
    "    N,data_len,data_dim = X_train.shape\n",
    "    ind_N = np.random.choice(N,batch_size,replace=False)\n",
    "    ind_start = 0#np.random.choice(data_len-num_steps,1)\n",
    "    #form batch\n",
    "    X_batch = X_train[ind_N,ind_start:ind_start+num_steps,:]\n",
    "    y_batch = y_train[ind_N]\n",
    "    return X_batch,y_batch\n",
    "\n",
    "def check_test(X_test,y_test,batch_size,num_steps):\n",
    "    \"\"\" Function to check the test_accuracy on the entire test set\n",
    "    This is a workaround. I haven't figured out yet how to make the graph\n",
    "    general for multiple batch sizes.\"\"\"\n",
    "    N = X_test.shape[0]\n",
    "    num_batch = np.floor(N/batch_size)\n",
    "    test_acc = np.zeros(num_batch)\n",
    "    for i in range(int(num_batch)):\n",
    "      X_batch, y_batch = sample_batch(X_test,y_test,batch_size,num_steps)\n",
    "      test_acc[i] = session.run(accuracy,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "    return np.mean(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Hyperparamaters\"\"\"\n",
    "init_scale = 0.08           #Initial scale for the states\n",
    "max_grad_norm = 25          #Clipping of the gradient before update\n",
    "num_layers = 2              #Number of stacked LSTM layers\n",
    "num_steps = 30              #Number of steps to backprop over at every batch\n",
    "hidden_size = 13            #Number of entries of the cell state of the LSTM\n",
    "max_iterations = 2000       #Maximum iterations to train\n",
    "batch_size = 2              #Batch size\n",
    "dropout = 0.8               # Keep probability of the dropout wrapper\n",
    "\n",
    "\n",
    "\"\"\"Place holders\"\"\"\n",
    "input_data = tf.placeholder(tf.float32, [None, num_steps,8], name = 'input_data')\n",
    "targets = tf.placeholder(tf.int64, [None], name='Targets')\n",
    "#Used later on for drop_out. At testtime, we pass 1.0\n",
    "keep_prob = tf.placeholder(\"float\", name = 'Drop_out_keep_prob')\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "with tf.variable_scope(\"model\", initializer=initializer):\n",
    "  \"\"\"Define the basis LSTM\"\"\"\n",
    "  with tf.name_scope(\"LSTM_setup\") as scope:\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)   #Initialize the zero_state. Note that it has to be run in session-time\n",
    "    #We have only one input dimension, but we generalize our code for future expansion\n",
    "    inputs = input_data#tf.expand_dims(input_data, 2)\n",
    "\n",
    "  #Define the recurrent nature of the LSTM\n",
    "  #Re-use variables only after first time-step\n",
    "  with tf.name_scope(\"LSTM\") as scope:\n",
    "    outputs = []\n",
    "    state = initial_state\n",
    "    with tf.variable_scope(\"LSTM_state\"):\n",
    "      for time_step in range(num_steps):\n",
    "       if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "       (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "       outputs.append(cell_output)       #Now cell_output is size [batch_size x hidden_size]\n",
    "    avg_output = tf.reduce_mean(tf.pack(outputs),0)\n",
    "    size1 = tf.shape(avg_output)\n",
    "    size2 = tf.shape(cell_output)\n",
    "\n",
    "\n",
    "#Generate a classification from the last cell_output\n",
    "#Note, this is where timeseries classification differs from sequence to sequence\n",
    "#modelling. We only output to Softmax at last time step\n",
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "  with tf.variable_scope(\"Softmax_params\"):\n",
    "    # Both datasets have four output classes. Improve the code by changing the 4\n",
    "    # into a hyperparameter\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, 4])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [4])\n",
    "  logits = tf.matmul(avg_output, softmax_w) + softmax_b\n",
    "  #Use sparse Softmax because we have mutually exclusive classes\n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,targets,name = 'Sparse_softmax')\n",
    "  cost = tf.reduce_sum(loss) / batch_size\n",
    "  #Pass on a summary to Tensorboard\n",
    "  cost_summ = tf.scalar_summary('Cost',cost)\n",
    "  # Calculate the accuracy\n",
    "  correct_prediction = tf.equal(tf.argmax(logits,1), targets)\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\"\"\"Optimizer\"\"\"\n",
    "with tf.name_scope(\"Optimizer\") as scope:\n",
    "  tvars = tf.trainable_variables()\n",
    "  #We clip the gradients to prevent explosion\n",
    "  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),max_grad_norm)\n",
    "  optimizer = tf.train.AdamOptimizer(8e-3)\n",
    "  gradients = zip(grads, tvars)\n",
    "  train_op = optimizer.apply_gradients(gradients)\n",
    "  # Add histograms for variables, gradients and gradient norms.\n",
    "  # The for-loop loops over all entries of the gradient and plots\n",
    "  # a histogram. We cut of\n",
    "  for gradient, variable in gradients:\n",
    "    if isinstance(gradient, ops.IndexedSlices):\n",
    "      grad_values = gradient.values\n",
    "    else:\n",
    "      grad_values = gradient\n",
    "    h1 = tf.histogram_summary(variable.name, variable)\n",
    "    h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n",
    "    h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "\n",
    "\"\"\"Load the data\"\"\"\n",
    "dummy = True\n",
    "if dummy:\n",
    "  data_train = np.loadtxt('train_data',delimiter=',')\n",
    "  data_test_val = np.loadtxt('test_data',delimiter=',')\n",
    "else:\n",
    "  data_train = np.loadtxt('data_train_dummy',delimiter=',')\n",
    "  data_test_val = np.loadtxt('data_test_dummy',delimiter=',')\n",
    "    \n",
    "data_train = format_data(data_train)\n",
    "data_test_val = format_data(data_test_val)\n",
    "\n",
    "# Split data into two equal halves for validation and test\n",
    "data_test,data_val = np.split(data_test_val,2)\n",
    "\n",
    "# Skip the last channel/feature_map as that pertains to the labels repeated for each of the time step (in columns)\n",
    "X_train = data_train[:,:,:-1]\n",
    "X_val = data_val[:,:,:-1]\n",
    "X_test = data_test[:,:,:-1]\n",
    "N = X_train.shape[0]\n",
    "Ntest = X_test.shape[0]\n",
    "\n",
    "# Copy the labels (As the labels remains same for all the columns for a given row, we only pick up the first)\n",
    "y_train = data_train[:,0,-1]\n",
    "y_val = data_val[:,0,-1]\n",
    "y_test = data_test[:,0,-1]\n",
    "\n",
    "print(\"Train data shape:\\nLabels shape:\",(X_train.shape, y_train.shape))\n",
    "\n",
    "#Final code for the TensorBoard\n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "# Collect the costs in a numpy fashion\n",
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train with approximately %d epochs' %(epochs))\n",
    "perf_collect = np.zeros((3,int(np.floor(max_iterations /100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Session time\"\"\"\n",
    "with tf.Session() as session:\n",
    "  writer = tf.train.SummaryWriter(\"logs/\", session.graph_def)\n",
    "  tf.initialize_all_variables().run()\n",
    "\n",
    "\n",
    "  step = 0\n",
    "  for i in range(max_iterations):\n",
    "\n",
    "    # Calculate some sizes\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    #Sample batch for training\n",
    "    X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)\n",
    "    state = initial_state.eval()  #Fire up the LSTM\n",
    "\n",
    "    #Next line does the actual training\n",
    "    session.run(train_op,feed_dict = {input_data: X_batch,targets: y_batch,initial_state: state,keep_prob:dropout})\n",
    "    if i==0:\n",
    "        # Uset this line to check before-and-after test accuracy\n",
    "        acc_test_before = check_test(X_test,y_test,batch_size,num_steps)\n",
    "        result = session.run([size1,size2],feed_dict = {input_data: X_batch,targets: y_batch,initial_state: state,keep_prob:dropout})\n",
    "        print(result[0])\n",
    "        print(result[1])\n",
    "    if i%100 == 0:\n",
    "      #Evaluate training performance\n",
    "      X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)\n",
    "      cost_out = session.run(cost,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      perf_collect[0,step] = cost_out\n",
    "      #print('At %d out of %d train cost is %.3f' %(i,max_iterations,cost_out)) #Uncomment line to follow train cost\n",
    "\n",
    "      #Evaluate validation performance\n",
    "      X_batch, y_batch = sample_batch(X_val,y_val,batch_size,num_steps)\n",
    "      result = session.run([cost,merged,accuracy],feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      cost_out = result[0]\n",
    "      perf_collect[1,step] = cost_out\n",
    "      acc_val = result[2]\n",
    "      perf_collect[2,step] = acc_val\n",
    "      print('At %d out of %d val cost is %.3f and val acc is %.3f' %(i,max_iterations,cost_out,acc_val))\n",
    "\n",
    "      #Write information to TensorBoard\n",
    "      summary_str = result[1]\n",
    "      writer.add_summary(summary_str, i)\n",
    "      writer.flush()\n",
    "\n",
    "      step +=1\n",
    "  acc_test = check_test(X_test,y_test,batch_size,num_steps)\n",
    "\n",
    "\"\"\"Additional plots\"\"\"\n",
    "print('The accuracy on the test data is %.3f, before training was %.3f' %(acc_test,acc_test_before))\n",
    "plt.plot(perf_collect[0],label='Train')\n",
    "plt.plot(perf_collect[1],label = 'Valid')\n",
    "plt.plot(perf_collect[2],label = 'Valid accuracy')\n",
    "plt.axis([0, step, 0, np.max(perf_collect)])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
